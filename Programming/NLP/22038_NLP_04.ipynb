{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Language Models\n",
    "N gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'dog', 'This', 'is', 'a', 'cat', 'I', 'love', 'my', 'cat', 'This', 'is', 'my', 'name']\n",
      "\n",
      "All the possible Bigrams are\n",
      "[('This', 'is'), ('is', 'a'), ('a', 'dog'), ('This', 'is'), ('is', 'a'), ('a', 'cat'), ('I', 'love'), ('love', 'my'), ('my', 'cat'), ('This', 'is'), ('is', 'my'), ('my', 'name')]\n",
      "\n",
      "Bigrams along with their frequency\n",
      "{('This', 'is'): 3, ('is', 'a'): 2, ('a', 'dog'): 1, ('a', 'cat'): 1, ('I', 'love'): 1, ('love', 'my'): 1, ('my', 'cat'): 1, ('is', 'my'): 1, ('my', 'name'): 1}\n",
      "\n",
      "Unigrams along with their frequency\n",
      "{'This': 3, 'is': 3, 'a': 2, 'I': 1, 'love': 1, 'my': 2}\n",
      "\n",
      "Bigrams along with their probability\n",
      "{('This', 'is'): 1.0, ('is', 'a'): 0.6666666666666666, ('a', 'dog'): 0.5, ('a', 'cat'): 0.5, ('I', 'love'): 1.0, ('love', 'my'): 1.0, ('my', 'cat'): 0.5, ('is', 'my'): 0.3333333333333333, ('my', 'name'): 0.5}\n",
      "\n",
      "The bigrams in the given sentence are\n",
      "[('This', 'is'), ('is', 'my'), ('my', 'cat')]\n"
     ]
    }
   ],
   "source": [
    "def readData():\n",
    "    data = ['This is a dog', 'This is a cat', 'I love my cat', 'This is my name']\n",
    "    dat = []\n",
    "    for i in range(len(data)):\n",
    "        for word in data[i].split():\n",
    "            dat.append(word)\n",
    "    print(dat)\n",
    "    return dat\n",
    "\n",
    "def createBigram(data):\n",
    "    listOfBigrams = []\n",
    "    bigramCounts = {}\n",
    "    unigramCounts = {}\n",
    "    for i in range(len(data) - 1):\n",
    "        if i < len(data) - 1 and data[i + 1].islower():\n",
    "            listOfBigrams.append((data[i], data[i + 1]))\n",
    "            if (data[i], data[i + 1]) in bigramCounts:\n",
    "                bigramCounts[(data[i], data[i + 1])] += 1\n",
    "            else:\n",
    "                bigramCounts[(data[i], data[i + 1])] = 1\n",
    "            if data[i] in unigramCounts:\n",
    "                unigramCounts[data[i]] += 1\n",
    "            else:\n",
    "                unigramCounts[data[i]] = 1\n",
    "    return listOfBigrams, unigramCounts, bigramCounts\n",
    "\n",
    "def calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\n",
    "    listOfProb = {}\n",
    "    for bigram in listOfBigrams:\n",
    "        word1 = bigram[0]\n",
    "        word2 = bigram[1]\n",
    "        listOfProb[bigram] = (bigramCounts.get(bigram)) / (unigramCounts.get(word1))\n",
    "    return listOfProb\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = readData()\n",
    "    listOfBigrams, unigramCounts, bigramCounts = createBigram(data)\n",
    "\n",
    "    print(\"\\nAll the possible Bigrams are\")\n",
    "    print(listOfBigrams)\n",
    "\n",
    "    print(\"\\nBigrams along with their frequency\")\n",
    "    print(bigramCounts)\n",
    "\n",
    "    print(\"\\nUnigrams along with their frequency\")\n",
    "    print(unigramCounts)\n",
    "\n",
    "    bigramProb = calcBigramProb(listOfBigrams, unigramCounts, bigramCounts)\n",
    "\n",
    "    print(\"\\nBigrams along with their probability\")\n",
    "    print(bigramProb)\n",
    "    \n",
    "    inputList = \"This is my cat\"\n",
    "    splt = inputList.split()\n",
    "    outputProb1 = 1\n",
    "    bilist = []\n",
    "\n",
    "    for i in range(len(splt) - 1):\n",
    "        bilist.append((splt[i], splt[i + 1]))\n",
    "\n",
    "    print(\"\\nThe bigrams in the given sentence are\")\n",
    "    print(bilist)\n",
    "    \n",
    "    for i in range(len(bilist)):\n",
    "        if bilist[i] in bigramProb:\n",
    "            outputProb1 *= bigramProb[bilist[i]]\n",
    "        else:\n",
    "            outputProb1 *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'public': 0.05555555555555555,\n",
       " 'European': 0.05555555555555555,\n",
       " 'Bank': 0.05555555555555555,\n",
       " 'price': 0.1111111111111111,\n",
       " 'emirate': 0.05555555555555555,\n",
       " 'overseas': 0.05555555555555555,\n",
       " 'newspaper': 0.05555555555555555,\n",
       " 'company': 0.16666666666666666,\n",
       " 'Turkish': 0.05555555555555555,\n",
       " 'increase': 0.05555555555555555,\n",
       " 'options': 0.05555555555555555,\n",
       " 'Higher': 0.05555555555555555,\n",
       " 'pound': 0.05555555555555555,\n",
       " 'Italian': 0.05555555555555555,\n",
       " 'time': 0.05555555555555555}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Create a placeholder for the model\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# Count frequency of co-occurrence\n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        model[(w1, w2)][w3] += 1\n",
    "\n",
    "# Transform the counts to probabilities\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] /= total_count\n",
    "\n",
    "dict(model[\"today\", \"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today the Turkish pipeline , Once that pipeline is paying 3 . 3 mln dlrs by 1991 , exercisable starting in the prior year ' s actions are fueling the chances of a higher multiple because they had received acceptances from Stewart ' s 1986 profits were affected by the U . S . Agriculture Department ' s deployment of Iranian rhetoric with continued hard times in the Gulf .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Starting words\n",
    "text = [\"today\", \"the\"]\n",
    "sentence_finished = False\n",
    "\n",
    "while not sentence_finished:\n",
    "    # Select a random probability threshold\n",
    "    r = random.random()\n",
    "    accumulator = 0.0\n",
    "\n",
    "    for word in model[tuple(text[-2:])].keys():\n",
    "        accumulator += model[tuple(text[-2:])][word]\n",
    "        # Select words that are above the probability threshold\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "    if text[-2:] == [None, None]:\n",
    "        sentence_finished = True\n",
    "\n",
    "# Join and print the generated text\n",
    "print(' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, GRU, Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import re\n",
    "\n",
    "data_text = \"\"\"The unanimous Declaration of the thirteen united States of America, When in\n",
    "We hold these truths to be self-evident, that all men are created equal, that they are endou\n",
    "He has refused his Assent to Laws, the most wholesome and necessary for the public good.\n",
    "He has forbidden his Governors to pass Laws of immediate and pressing importance, unless su:\n",
    "He has refused to pass other Laws for the accommodation of large districts of people, unles:\n",
    "He has called together legislative bodies at places unusual, uncomfortable, and distant fror\n",
    "He has dissolved Representative Houses repeatedly, for opposing with manly firmness his inv\n",
    "He has refused for a long time, after such dissolutions, to cause others to be elected; whel\n",
    "He has endeavoured to prevent the population of these States; for that purpose obstructing 1\n",
    "He has obstructed the Administration of Justice, by refusing his Assent to Laws for establi:\n",
    "He has made Judges dependent on his Will alone, for the tenure of their offices, and the amc\n",
    "He has erected a multitude of New Offices, and sent hither swarms of Officers to harrass oul\n",
    "He has kept among us, in times of peace, Standing Armies without the Consent of our legislat\n",
    "He has affected to render the Military independent of and superior to the Civil power.\n",
    "He has combined with others to subject us to a jurisdiction foreign to our constitution, an\n",
    "For Quartering large bodies of armed troops among us:\n",
    "For protecting them, by a mock Trial, from punishment for any Murders which they should comr\n",
    "For cutting off our Trade with all parts of the world:\n",
    "For imposing Taxes on us without our Consent:\n",
    "For depriving us in many cases, of the benefits of Trial by Jury:\n",
    "For transporting us beyond Seas to be tried for pretended offences\n",
    "For abolishing the free System of English Laws in a neighbouring Province, establishing thel\n",
    "For taking away our Charters, abolishing our most valuable Laws, and altering fundamentally\n",
    "For suspending our own Legislatures, and declaring themselves invested with power to legisl\n",
    "He has abdicated Government here, by declaring us out of his Protection and waging War agair\n",
    "He has plundered our seas, ravaged our Coasts, burnt our towns, and destroyed the lives of (\n",
    "He is at this time transporting large Armies of foreign Mercenaries to compleat the works o\n",
    "He has constrained our fellow Citizens taken Captive on the high Seas to bear Arms against 1\n",
    "He has excited domestic insurrections amongst us, and has endeavoured to bring on the inhab:\n",
    "In every stage of these Oppressions We have Petitioned for Redress in the most humble terms\n",
    "Nor have We been wanting in attentions to our Brittish brethren. We have warned them from t:\n",
    "We, therefore, the Representatives of the united States of America, in General Congress, As:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_cleaner(text):\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(r\"'s\\b\", \"\", newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    long_words = []\n",
    "    for i in newString.split():\n",
    "        if len(i) >= 3:\n",
    "            long_words.append(i)\n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n",
    "data_new = text_cleaner(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 2366\n"
     ]
    }
   ],
   "source": [
    "def create_seq(text):\n",
    "    length = 30\n",
    "    sequences = list()\n",
    "    for i in range(length, len(text)):\n",
    "        seq = text[i-length:i+1]\n",
    "        sequences.append(seq)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    return sequences\n",
    "\n",
    "# create sequences\n",
    "sequences = create_seq(data_new)\n",
    "# Total Sequences: 7052\n",
    "\n",
    "chars = sorted(list(set(data_new)))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "def encode_seq(seq, mapping):\n",
    "    encoded_seqs = []\n",
    "    for line in seq:\n",
    "        encoded_seq = [mapping[char] for char in line]\n",
    "        encoded_seqs.append(encoded_seq)\n",
    "    return np.array(encoded_seqs)\n",
    "\n",
    "# encode the sequences\n",
    "sequences = encode_seq(sequences, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20,  8,  5, ...,  5,  0, 20],\n",
       "       [ 8,  5,  0, ...,  0, 20,  8],\n",
       "       [ 5,  0, 21, ..., 20,  8,  9],\n",
       "       ...,\n",
       "       [ 4,  0, 19, ...,  7, 18,  5],\n",
       "       [ 0, 19, 20, ..., 18,  5, 19],\n",
       "       [19, 20,  1, ...,  5, 19, 19]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y\n",
    "X, y = sequences[:, :-1], sequences[:, -1]\n",
    "\n",
    "# one hot encode y\n",
    "y = to_categorical(y, num_classes=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2129, 30) Val shape: (237, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# vocabulary size\n",
    "vocab = len(mapping)\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "# create train and validation sets\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2129, 30) Val shape: (237, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Reshape sequences to make it 2-dimensional\n",
    "sequences = sequences.reshape((len(sequences), sequences.shape[1]))\n",
    "\n",
    "# create X and y\n",
    "X, y = sequences[:, :-1], sequences[:, -1]\n",
    "\n",
    "# one hot encode y\n",
    "y = to_categorical(y, num_classes=vocab)\n",
    "\n",
    "# create train and validation sets\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/100\n",
      "67/67 - 5s - 74ms/step - acc: 0.1357 - loss: 2.9802 - val_acc: 0.1941 - val_loss: 2.8627\n",
      "Epoch 2/100\n",
      "67/67 - 1s - 13ms/step - acc: 0.2006 - loss: 2.7167 - val_acc: 0.2574 - val_loss: 2.5786\n",
      "Epoch 3/100\n",
      "67/67 - 1s - 14ms/step - acc: 0.2748 - loss: 2.4241 - val_acc: 0.2658 - val_loss: 2.4154\n",
      "Epoch 4/100\n",
      "67/67 - 1s - 21ms/step - acc: 0.3062 - loss: 2.3065 - val_acc: 0.2869 - val_loss: 2.3754\n",
      "Epoch 5/100\n",
      "67/67 - 2s - 26ms/step - acc: 0.3255 - loss: 2.2305 - val_acc: 0.2954 - val_loss: 2.3475\n",
      "Epoch 6/100\n",
      "67/67 - 2s - 30ms/step - acc: 0.3419 - loss: 2.1605 - val_acc: 0.3207 - val_loss: 2.2889\n",
      "Epoch 7/100\n",
      "67/67 - 2s - 37ms/step - acc: 0.3537 - loss: 2.1076 - val_acc: 0.3165 - val_loss: 2.2679\n",
      "Epoch 8/100\n",
      "67/67 - 2s - 32ms/step - acc: 0.3776 - loss: 2.0545 - val_acc: 0.3165 - val_loss: 2.2564\n",
      "Epoch 9/100\n",
      "67/67 - 2s - 36ms/step - acc: 0.3894 - loss: 2.0051 - val_acc: 0.3249 - val_loss: 2.2812\n",
      "Epoch 10/100\n",
      "67/67 - 2s - 31ms/step - acc: 0.3969 - loss: 1.9631 - val_acc: 0.3165 - val_loss: 2.2532\n",
      "Epoch 11/100\n",
      "67/67 - 2s - 28ms/step - acc: 0.4157 - loss: 1.9144 - val_acc: 0.3165 - val_loss: 2.2397\n",
      "Epoch 12/100\n",
      "67/67 - 3s - 38ms/step - acc: 0.4223 - loss: 1.8707 - val_acc: 0.3418 - val_loss: 2.2290\n",
      "Epoch 13/100\n",
      "67/67 - 3s - 43ms/step - acc: 0.4457 - loss: 1.8274 - val_acc: 0.3122 - val_loss: 2.2393\n",
      "Epoch 14/100\n",
      "67/67 - 2s - 29ms/step - acc: 0.4509 - loss: 1.7753 - val_acc: 0.3502 - val_loss: 2.2436\n",
      "Epoch 15/100\n",
      "67/67 - 2s - 28ms/step - acc: 0.4674 - loss: 1.7084 - val_acc: 0.3376 - val_loss: 2.2276\n",
      "Epoch 16/100\n",
      "67/67 - 2s - 26ms/step - acc: 0.4894 - loss: 1.6566 - val_acc: 0.3671 - val_loss: 2.2254\n",
      "Epoch 17/100\n",
      "67/67 - 2s - 28ms/step - acc: 0.4974 - loss: 1.6138 - val_acc: 0.3544 - val_loss: 2.2430\n",
      "Epoch 18/100\n",
      "67/67 - 2s - 25ms/step - acc: 0.5186 - loss: 1.5408 - val_acc: 0.3629 - val_loss: 2.2295\n",
      "Epoch 19/100\n",
      "67/67 - 2s - 27ms/step - acc: 0.5317 - loss: 1.4882 - val_acc: 0.3671 - val_loss: 2.2843\n",
      "Epoch 20/100\n",
      "67/67 - 2s - 33ms/step - acc: 0.5604 - loss: 1.4178 - val_acc: 0.3840 - val_loss: 2.2753\n",
      "Epoch 21/100\n",
      "67/67 - 2s - 27ms/step - acc: 0.5777 - loss: 1.3623 - val_acc: 0.3671 - val_loss: 2.3001\n",
      "Epoch 22/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.5885 - loss: 1.3092 - val_acc: 0.3882 - val_loss: 2.3062\n",
      "Epoch 23/100\n",
      "67/67 - 2s - 27ms/step - acc: 0.6116 - loss: 1.2365 - val_acc: 0.3797 - val_loss: 2.3116\n",
      "Epoch 24/100\n",
      "67/67 - 2s - 31ms/step - acc: 0.6473 - loss: 1.1719 - val_acc: 0.3629 - val_loss: 2.3436\n",
      "Epoch 25/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.6557 - loss: 1.1159 - val_acc: 0.3586 - val_loss: 2.3891\n",
      "Epoch 26/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.6721 - loss: 1.0532 - val_acc: 0.3671 - val_loss: 2.4001\n",
      "Epoch 27/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.6891 - loss: 0.9995 - val_acc: 0.3671 - val_loss: 2.3996\n",
      "Epoch 28/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.7130 - loss: 0.9490 - val_acc: 0.3629 - val_loss: 2.4551\n",
      "Epoch 29/100\n",
      "67/67 - 2s - 25ms/step - acc: 0.7276 - loss: 0.8860 - val_acc: 0.3755 - val_loss: 2.4714\n",
      "Epoch 30/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.7417 - loss: 0.8471 - val_acc: 0.3713 - val_loss: 2.4808\n",
      "Epoch 31/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.7656 - loss: 0.7896 - val_acc: 0.3924 - val_loss: 2.5457\n",
      "Epoch 32/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.7741 - loss: 0.7582 - val_acc: 0.3629 - val_loss: 2.5692\n",
      "Epoch 33/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.8027 - loss: 0.7008 - val_acc: 0.3671 - val_loss: 2.5672\n",
      "Epoch 34/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.8116 - loss: 0.6699 - val_acc: 0.3840 - val_loss: 2.5626\n",
      "Epoch 35/100\n",
      "67/67 - 2s - 25ms/step - acc: 0.8088 - loss: 0.6432 - val_acc: 0.3840 - val_loss: 2.6370\n",
      "Epoch 36/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.8309 - loss: 0.6048 - val_acc: 0.3755 - val_loss: 2.6572\n",
      "Epoch 37/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.8497 - loss: 0.5588 - val_acc: 0.3882 - val_loss: 2.6910\n",
      "Epoch 38/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.8525 - loss: 0.5462 - val_acc: 0.3882 - val_loss: 2.7394\n",
      "Epoch 39/100\n",
      "67/67 - 2s - 26ms/step - acc: 0.8694 - loss: 0.4996 - val_acc: 0.3882 - val_loss: 2.7509\n",
      "Epoch 40/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.8675 - loss: 0.4808 - val_acc: 0.3797 - val_loss: 2.8078\n",
      "Epoch 41/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.8812 - loss: 0.4540 - val_acc: 0.3797 - val_loss: 2.7981\n",
      "Epoch 42/100\n",
      "67/67 - 2s - 25ms/step - acc: 0.8915 - loss: 0.4279 - val_acc: 0.3840 - val_loss: 2.8343\n",
      "Epoch 43/100\n",
      "67/67 - 1s - 22ms/step - acc: 0.8962 - loss: 0.4085 - val_acc: 0.3755 - val_loss: 2.8910\n",
      "Epoch 44/100\n",
      "67/67 - 1s - 22ms/step - acc: 0.8910 - loss: 0.3987 - val_acc: 0.3882 - val_loss: 2.8989\n",
      "Epoch 45/100\n",
      "67/67 - 1s - 22ms/step - acc: 0.9079 - loss: 0.3692 - val_acc: 0.3924 - val_loss: 2.8903\n",
      "Epoch 46/100\n",
      "67/67 - 1s - 21ms/step - acc: 0.9112 - loss: 0.3572 - val_acc: 0.3755 - val_loss: 2.9576\n",
      "Epoch 47/100\n",
      "67/67 - 1s - 21ms/step - acc: 0.9159 - loss: 0.3445 - val_acc: 0.3755 - val_loss: 2.9866\n",
      "Epoch 48/100\n",
      "67/67 - 1s - 22ms/step - acc: 0.9239 - loss: 0.3234 - val_acc: 0.3840 - val_loss: 3.0451\n",
      "Epoch 49/100\n",
      "67/67 - 2s - 22ms/step - acc: 0.9272 - loss: 0.3135 - val_acc: 0.3797 - val_loss: 3.0723\n",
      "Epoch 50/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9300 - loss: 0.3011 - val_acc: 0.3797 - val_loss: 3.0978\n",
      "Epoch 51/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9352 - loss: 0.2793 - val_acc: 0.3713 - val_loss: 3.1437\n",
      "Epoch 52/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9319 - loss: 0.2711 - val_acc: 0.3840 - val_loss: 3.1851\n",
      "Epoch 53/100\n",
      "67/67 - 1s - 22ms/step - acc: 0.9310 - loss: 0.2747 - val_acc: 0.3713 - val_loss: 3.1657\n",
      "Epoch 54/100\n",
      "67/67 - 1s - 22ms/step - acc: 0.9408 - loss: 0.2427 - val_acc: 0.3629 - val_loss: 3.1825\n",
      "Epoch 55/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9385 - loss: 0.2515 - val_acc: 0.3797 - val_loss: 3.2221\n",
      "Epoch 56/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9432 - loss: 0.2442 - val_acc: 0.3840 - val_loss: 3.2553\n",
      "Epoch 57/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9497 - loss: 0.2280 - val_acc: 0.3755 - val_loss: 3.2766\n",
      "Epoch 58/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9469 - loss: 0.2098 - val_acc: 0.3924 - val_loss: 3.2752\n",
      "Epoch 59/100\n",
      "67/67 - 1s - 22ms/step - acc: 0.9516 - loss: 0.2153 - val_acc: 0.3797 - val_loss: 3.2863\n",
      "Epoch 60/100\n",
      "67/67 - 1s - 22ms/step - acc: 0.9535 - loss: 0.2026 - val_acc: 0.3840 - val_loss: 3.3041\n",
      "Epoch 61/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9465 - loss: 0.2002 - val_acc: 0.3840 - val_loss: 3.2934\n",
      "Epoch 62/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9488 - loss: 0.2004 - val_acc: 0.3755 - val_loss: 3.3422\n",
      "Epoch 63/100\n",
      "67/67 - 1s - 22ms/step - acc: 0.9601 - loss: 0.1862 - val_acc: 0.3797 - val_loss: 3.3911\n",
      "Epoch 64/100\n",
      "67/67 - 1s - 22ms/step - acc: 0.9596 - loss: 0.1852 - val_acc: 0.3755 - val_loss: 3.4322\n",
      "Epoch 65/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9577 - loss: 0.1805 - val_acc: 0.3840 - val_loss: 3.4655\n",
      "Epoch 66/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9577 - loss: 0.1790 - val_acc: 0.3797 - val_loss: 3.5083\n",
      "Epoch 67/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9624 - loss: 0.1633 - val_acc: 0.3755 - val_loss: 3.5003\n",
      "Epoch 68/100\n",
      "67/67 - 2s - 25ms/step - acc: 0.9732 - loss: 0.1474 - val_acc: 0.3671 - val_loss: 3.5033\n",
      "Epoch 69/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.9638 - loss: 0.1520 - val_acc: 0.3755 - val_loss: 3.5617\n",
      "Epoch 70/100\n",
      "67/67 - 2s - 25ms/step - acc: 0.9685 - loss: 0.1539 - val_acc: 0.3671 - val_loss: 3.5928\n",
      "Epoch 71/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.9667 - loss: 0.1427 - val_acc: 0.3797 - val_loss: 3.5696\n",
      "Epoch 72/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9624 - loss: 0.1515 - val_acc: 0.3755 - val_loss: 3.6374\n",
      "Epoch 73/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.9685 - loss: 0.1333 - val_acc: 0.3797 - val_loss: 3.5999\n",
      "Epoch 74/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.9704 - loss: 0.1292 - val_acc: 0.3671 - val_loss: 3.6484\n",
      "Epoch 75/100\n",
      "67/67 - 2s - 25ms/step - acc: 0.9737 - loss: 0.1224 - val_acc: 0.3713 - val_loss: 3.6168\n",
      "Epoch 76/100\n",
      "67/67 - 2s - 30ms/step - acc: 0.9605 - loss: 0.1465 - val_acc: 0.3629 - val_loss: 3.6901\n",
      "Epoch 77/100\n",
      "67/67 - 2s - 29ms/step - acc: 0.9695 - loss: 0.1321 - val_acc: 0.3586 - val_loss: 3.7281\n",
      "Epoch 78/100\n",
      "67/67 - 2s - 28ms/step - acc: 0.9723 - loss: 0.1263 - val_acc: 0.3544 - val_loss: 3.7830\n",
      "Epoch 79/100\n",
      "67/67 - 2s - 27ms/step - acc: 0.9789 - loss: 0.1132 - val_acc: 0.3544 - val_loss: 3.7896\n",
      "Epoch 80/100\n",
      "67/67 - 2s - 28ms/step - acc: 0.9728 - loss: 0.1231 - val_acc: 0.3755 - val_loss: 3.7569\n",
      "Epoch 81/100\n",
      "67/67 - 2s - 28ms/step - acc: 0.9713 - loss: 0.1232 - val_acc: 0.3797 - val_loss: 3.7696\n",
      "Epoch 82/100\n",
      "67/67 - 2s - 27ms/step - acc: 0.9718 - loss: 0.1200 - val_acc: 0.3797 - val_loss: 3.7981\n",
      "Epoch 83/100\n",
      "67/67 - 2s - 28ms/step - acc: 0.9723 - loss: 0.1156 - val_acc: 0.3797 - val_loss: 3.7758\n",
      "Epoch 84/100\n",
      "67/67 - 2s - 26ms/step - acc: 0.9723 - loss: 0.1097 - val_acc: 0.3797 - val_loss: 3.8437\n",
      "Epoch 85/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9737 - loss: 0.1128 - val_acc: 0.3755 - val_loss: 3.8476\n",
      "Epoch 86/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9793 - loss: 0.1048 - val_acc: 0.3671 - val_loss: 3.8681\n",
      "Epoch 87/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9770 - loss: 0.1066 - val_acc: 0.3671 - val_loss: 3.8831\n",
      "Epoch 88/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9713 - loss: 0.1117 - val_acc: 0.3755 - val_loss: 3.9836\n",
      "Epoch 89/100\n",
      "67/67 - 2s - 22ms/step - acc: 0.9826 - loss: 0.0958 - val_acc: 0.3629 - val_loss: 3.9705\n",
      "Epoch 90/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.9784 - loss: 0.0988 - val_acc: 0.3713 - val_loss: 3.9953\n",
      "Epoch 91/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9789 - loss: 0.0950 - val_acc: 0.3797 - val_loss: 4.0047\n",
      "Epoch 92/100\n",
      "67/67 - 1s - 22ms/step - acc: 0.9784 - loss: 0.0899 - val_acc: 0.3797 - val_loss: 3.9977\n",
      "Epoch 93/100\n",
      "67/67 - 2s - 22ms/step - acc: 0.9732 - loss: 0.1002 - val_acc: 0.3713 - val_loss: 4.0118\n",
      "Epoch 94/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9775 - loss: 0.0893 - val_acc: 0.3586 - val_loss: 3.9925\n",
      "Epoch 95/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9765 - loss: 0.0937 - val_acc: 0.3671 - val_loss: 4.0237\n",
      "Epoch 96/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9732 - loss: 0.1036 - val_acc: 0.3840 - val_loss: 3.9908\n",
      "Epoch 97/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9807 - loss: 0.0877 - val_acc: 0.3797 - val_loss: 4.0322\n",
      "Epoch 98/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9798 - loss: 0.0846 - val_acc: 0.3966 - val_loss: 4.0919\n",
      "Epoch 99/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.9803 - loss: 0.0863 - val_acc: 0.3755 - val_loss: 4.1021\n",
      "Epoch 100/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.9822 - loss: 0.0815 - val_acc: 0.3713 - val_loss: 4.1010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2941d6cf310>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab, 50, input_length=30, trainable=True))\n",
    "model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\n",
    "model.add(Dense(vocab, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "model.fit(X_tr, y_tr, epochs=100, verbose=2, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "large armies offices and sent \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of characters\n",
    "    for _ in range(n_chars):\n",
    "        # encode the characters as integers\n",
    "        encoded = [mapping[char] for char in in_text]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict character\n",
    "        yhat = model.predict(encoded, verbose=0)\n",
    "        # get the index of the character with the highest probability\n",
    "        yhat = np.argmax(yhat)\n",
    "        # reverse map integer to character\n",
    "        out_char = ''\n",
    "        for char, index in mapping.items():\n",
    "            if index == yhat:\n",
    "                out_char = char\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += out_char\n",
    "    return in_text\n",
    "\n",
    "inp = 'Large armies of'\n",
    "print(len(inp))\n",
    "print(generate_seq(model, mapping, 50, inp.lower(), 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference:\n",
    "In this NLP lab, we covered several techniques for word representation, starting with One-Hot Encoding. We transformed a given text into lowercase, tokenized it, obtained unique words, assigned integer positions to words, and created a one-hot matrix. Next, we explored Bag of Words using CountVectorizer, applying it to example sentences. We proceeded to Tf-Idf (term frequency-inverse document frequency), constructing a small corpus and computing TF and IDF values. We also delved into Word2Vec, leveraging the gensim library to download and use a pre-trained model, demonstrating word embeddings and similarity calculations. Additionally, we explored GloVe, utilizing spaCy to load a pre-trained model and showcasing word embeddings for sample sentences. Finally, we implemented text embedding using GloVe for custom sentences and visualized word embeddings in 2D space using t-SNE. These techniques enhance our understanding of how to represent words in a format suitable for natural language processing tasks. In addition to this, I have implemented an n-gram language model specifically designed for predictive next-word tasks. This involved considering sequences of n words within the text to discern patterns and relationships, enabling the model to predict the next word in a given context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
